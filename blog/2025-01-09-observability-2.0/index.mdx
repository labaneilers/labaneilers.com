---
slug: are-we-ready-for-observability-2.0
title: Are we ready for Observability 2.0?
tags: [devops, platform-engineering, kubernetes]
image: ./bee.jpg
draft: true
---

:::tip[TL;DR]
Observability 2.0 is a vision of observability that seeks to replace the traditional "three pillars" of observability (metrics, logs, and traces) with a single source of truth: wide events. 

This vision is compelling, but there are a number of obstacles that make it difficult to adopt in practice.
:::

import Bee from './bee.jpg';

<img src={Bee} className="blog-image" alt="A bee looking through a telescope"/>

At SimpliSafe, we manage a pretty large system of microservices. Because we're entrusted by our customers to protect their homes and families, we take reliability of our systems pretty seriously. Observability is an essential part of building and maintaining a system like this, and over the last couple years, we've been actively investing in our capabilities.

As of last year, we were using separate tools for each of the "three pillars": [Grafana Cloud](https://grafana.com/products/cloud/metrics/) for metrics, [Honeycomb](https://www.honeycomb.io/) for traces, and [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) for logs. Of these three, OpenSearch had become the biggest pain point (for soooo many reasons), and we were looking at some better alternatives. 

Because we were already using Grafana Cloud for metrics, we decided to evaluate their [logs product](https://grafana.com/products/cloud/grafana-logs/) first, knowing that Grafana Labs has invested a lot in providing UIs that integrate observability signals. After an evaluation, we were pretty satisfied with Loki (it was certainly a lot better than OpenSearch), but we thought we should also check in with Honeycomb, who had recently introduced a [new logs product](https://www.honeycomb.io/log-analytics).

In our conversation with Honeycomb, they ended up making a pretty interesting case that we should consider going all in on traces... *in lieu of logs*. We had up to this point been thinking very incrementally, and here was Honeycomb, coming in hot with a bold and revolutionary vision: Observability 2.0.

{/* truncate */}

### WTF is Observability 2.0?

The term "[Observability 2.0](https://www.honeycomb.io/blog/one-key-difference-observability1dot0-2dot0
)" was coined by Charity Majors (the CTO and co-founder of Honeycomb) as a shorthand for a new vision of observability, defined in opposition to the "three pillars" model. 

:::tip charity.wtf
If you're somehow one of the 5 technology people on the internet that isn't already aware of [Charity's blog](https://charity.wtf/), you should really stop reading this drivel and head there immediately. She's a fountain of insight on technology, engineering management, observability, and many other topics.

Also, I love her writing style, and her use of profanity as a persuasive device is fucking delightful.  
:::

Here's the gist of Observability 2.0:

* Collect telemetry in the form "wide events" (usually traces, but also maybe structured logs) which can contain an arbitrary number of key/value pairs with high cardinality values
* Your observability tooling should allow you to query these events to answer arbitrarily complex questions about your system

In this vision, traditional metrics (i.e. timeseries emitted directly from your apps) are considerably less valuable, because they, by definition, are stripped of context when they're aggregated into timeseries. Since cardinality is the main driver of cost in a timeseries database, you have to choose which attributes you care about (ensuring they're low/moderate cardinality), and drop the rest. This means you also have to know what questions you'll want to ask of your system *in advance of deploying the code*. 

With wide events, on the other hand, you keep all that rich, high-cardinality data, and then you get to slice and dice it in ways you couldn't have predicted you'd need in advance. You can use wide events to derive new metrics on the fly, and use the rich attributes to dig into specific anomalies in ways that are impractical with pre-aggregated timeseries data.

Furthermore, in Observability 2.0, logs become somewhat redundant with traces, too. Traces are an ideal superset of logs: structured events, but with additional conventions around how they represent units of work and their relationships (i.e. spans with durations, parents, siblings, and children). 

This is a pretty compelling vision: one universal source of truth for all your observability data. You don't have to suffer the cost of 3 pillars when you can derive all the same value from wide events, and you get to simplify your whole practice around a smaller, more powerful set of tools.

Rock and roll! This sounds amazing! Sign me up!

### The gap between vision and reality

To be fair, the idea of Observability 2.0 is something my colleagues at SimpliSafe and I been thinking about for a while now. Charity has written and spoken a whole lot about it, and because of the elegance of the idea (and her fantastic writing), we already understood the core ideas, and had experience with Honeycomb, so we were primed to consider something a bit more forward thinking.

When Honeycomb came back to us with a concrete plan, we realized it was finally time to take off our incrementalist hats and start considering what it would really mean to go all in on traces as the source of truth.

It didn't take long for us to start enumerating concerns.

### In defense of traditional metrics 

I may have oversimplified the argument against metrics; Charity's actual words are a bit nuanced. For instance, she still sees value in using metrics to monitor infrastructure (especially third party infra). It's just that for the code that you're writing yourself- at the core of your business- and is changing constantly, metrics alone aren't sufficient to understand what's going on.

This is absolutely true. Metrics *alone* are insufficient to really understand the behavior of your system. You absolutely need high-cardinality data such as logs and traces to answer many types of questions. 

But I would argue that traditional metrics are still necessary as a *complement* to wide events due to the huge and fundamental cost discrepancy between the two.

### Let's talk about cost

The reason we still need metrics comes down to the *enormous* discrepancy in cost between metrics vs wide events. Metrics are *extraordinarily cheap*. Like a *fraction* of the cost of logs or traces, and the comparison gets more and more one-sided as you scale up.

* Timeseries metrics are incredibly compact and lightweight. Cost is driven by the number of timeseries you're producing: unique combinations of label/value pairs (e.g. `pod`, `request_path`, `response_code`). These pairs get stored once, and then the rest of the cost is basically just a number stored every interval (30 seconds or so).
The tradeoff is that you lose context of the individual events from which the aggregate measurement was derived.
* Wide events are fundamentally a more heavyweight signal than metrics, since each event is basically its own bag of string attributes. Vendors generally charge for each span/event (or by number of bytes) ingested. You also pay a cost at runtime: there's more data to generate, encode, buffer, and transmit. 

:::info wide events, caviar, and private islands
And lets just be clear, if money were no object, **of course** I'd rather have full-fidelity wide events than metrics for everything! 

I'd also prefer to dine exclusively in restaurants with Michelin stars, but I don't have the means to do that either.
::: 




### Let's compare costs as you scale

As the frequency/volume of events increases, the fundamental overhead of wide events grows increasingly impractical in terms of computation, network, and storage... and ultimately dollars. This is not only because wide events are more heavyweight, but because there's *no aggregation*, so there's nothing to bend the curve in your favor as your load increases.

For example: imagine an operation that happens 1000 times per second. You can instrument this with a single exponential histogram metric, at the cost of a few active series (lets say 10 for the sake of argument). With traces, you're producing *1000 spans per second* for the same operation.

* With metrics, you have 10 active timeseries. Using [list pricing for Grafana Cloud](https://grafana.com/pricing/) of $8 for 1K series, this will cost **a few cents per month**.
* With traces, you're producing *1000 spans per second*. Using [list pricing for Honeycomb](https://www.honeycomb.io/pricing) of 100M events for $130, this will cost **~$3300 per month**.

Now let's scale this up by having 10 pods each handling 1000 requests per second. 

* With metrics, you'll now have 100 active timeseries, which will cost **a few cents per month**.
* With traces, you're now producing *10,000 spans per second*, which will cost **~$33K per month**.

I wasn't kidding, right?

Having the ability to choose metrics vs wide events for a given use-case can give you a lot more flexibility to find a good balance between the richness of telemetry data and its cost.

:::tip Ingest-time aggregation is a thing!
There's now a number of new, ingest-time aggregation tools (e.g. [Grafana Cloud's Adaptive Metrics](https://grafana.com/docs/grafana-cloud/cost-management-and-billing/reduce-costs/metrics-costs/control-metrics-usage-via-adaptive-metrics/) or [Chronosphere's aggregation rules](https://docs.chronosphere.io/control/shaping/rules)) which can drastically reduce active series by aggregating away unneeded high-cardinality labels.

For example, if you only cared about the aggregate request count across all pods (and not *per pod*), you could aggregate away the `pod` label, and you'd suddenly have *one single series* to track all instances for a given operation.

This makes metrics even *more* cost effective.
:::

### Wait, so how is *anybody* using wide events?

In our case, the cost of capturing all our traces and using them to derive metrics would be an *order of magnitude* more expensive than what we're currently paying for all our timeseries metrics.

But wait, you ask... surely customers are using Honeycomb and other tracing products and they're not paying this absurd cost. 

Yep- they're probably doing what we're doing with our traces: *sampling*.


### Parents, have you talked to your kids about sampling traces?

Sampling is a whole topic in itself; the gist is that you randomly select one out of every N traces, and rely on statistical extrapolation when you run queries. The attributes necessary to do sampling are actually built into the OpenTelemetry spec. 

There's some tooling available, which can use these attributes, to do [tail sampling](https://opentelemetry.io/docs/concepts/sampling/#tail-sampling) (we use Honeycomb's [Refinery](https://github.com/honeycombio/refinery) for this). Honeycomb also does a nice job of extrapolating sampled values at query time based on the sample ratio, so it feels pretty opaque to a user- sometimes you forget you're working with sampled spans.

We're currently sampling at a 30/1 ratio, which, at our volume, is *usually* sufficient to get accurate enough aggregate measurements.

:::warning What's your sampling ratio?
I had dinner recently at an Observability community event, and met a nice guy in a similar position to mine. When I found out he was using Honeycomb too, I asked him what sampling ratio they were using. 

After I asked, I suddenly felt awkward, like maybe that was too personal a question to ask someone I'd just met.
:::

import Awkward from './awkward.png';

<img src={Awkward} alt="Asking someone what sampling ratio they use with Honeycomb"/>

#### Sampling is complicated and error prone

Theres a *lot* of moving parts required to make sampling work at scale. All it takes is one programmer in one service to make a mistake with an attribute that controls sampling behavior, and you can end up with whole classes of missing traces, or conversely, accidentally disabling sampling and blowing up your bill. We've had both of these happen, and in a couple cases, we've had queries return *very* misleading results.

Luckily, in these cases, we had traditional metrics available that directly contradicted the wrong conclusions we were drawing from the traces, and saved us from making some pretty bad decisions.

So if you're doing sampling on your wide events, traditional timeseries metrics are *extremely valuable* for corroborating results from your queries.

:::info Mea culpa
Just to be clear, these misleading results weren't Honeycomb's fault- it was errors on our side related to the subtleties of OTEL sampling configuration.
:::

#### Sampling breaks forensic use cases

The other big problem with sampling is that it makes it virtually impossible to rely on traces for forensic/diagnostic use cases. If you're trying to diagnose an issue for a specific customer, client device, or other specific request, the chances of having a trace available is slim.

Seriously, imagine trying to investigate a potential security breach with anything other than 100% of logs/traces.

So for these forensic use cases, we end up relying on traditional structured logs, and not traces.


### The historical inertia of metrics

I'll finish up my defense of metrics with a couple other *eminently practical* reasons which make it hard to imagine giving up traditional metrics entirely: 

* Metrics are boring, simple, tried and true. Virtually every mature industry uses metrics to drive their business and operations.
* Metrics are ubiquitous. Everything supports metrics. Most every tool in the cloud-native ecosystem exposes Prometheus metrics, and very few emit traces (though this is slowly changing). Cloud providers also expose telemetry primarily as metrics (e.g. CloudWatch, Azure Monitor, etc).

### Logs vs traces

While Charity's Observability 2.0 vision of wide events technically includes structured logs, they generally play second fiddle to traces. Let's quickly enumerate some of the reasons why logs are still valuable:

* Logs are simple and ubiquitous. Everything writes logs. Also all OSS/third-party infra produce logs, and very few produce traces.
* The developer experience for logs is dead simple, and great by default. Use `printf()`, start your app from a terminal, and watch the logs flow through stdout. It's a beautiful thing.
* Compared to traces, logs are a much more natural way to model events which aren't tied to requests made across services (e.g. lifecycle events, background jobs, etc).

And then there's just the sheer maturity and battle-tested nature of logs. For example, if you've gone all in on traces, where would you look to diagnose an app crashing? OTEL instrumentation isn't going to politely wrap up the current span and flush its buffer when my app panics. You're just going to lose any record of the cause of the panic.

### The path forward for Observability 2.0

I'm not writing this to dump on either tracing, or the overall vision of Observability 2.0. I'm still a huge fan of the idea of simplifying signals down to minimize telemetry sprawl, and I've experienced the benefits of tracing and wide events firsthand. 

Honeycomb is indeed a fucking *pleasure* to use- it's super fast, very powerful, and intuitive. Honeycomb's mere existence has prompted a virtual tidal wave of innovation across the observability space as competitors struggle to react to its power and elegance.

But despite how seductive the vision of Observability 2.0 is, when it came time to make a bold decision to go all in... we equivocated, put our incrementalist hats back on, and decided to continue with the three pillars for the time being. Observability 2.0 is just a *bit too cutting edge* at the moment.

I suspect these obstacles that prevented us from saying "yes" today are all solvable in time. I hear there are companies that have gone all in on Observability 2.0, and I've got to believe it's possible.

To close the gap for a company like SimpliSafe, here's the problems we'd need solved:

#### Affordability 

We've got to make wide events more cost efficient and affordable. Until we can get the costs down, sampling will continue to be necessary, and sampling undermines the idea that we can discard the three-pillars model:

* Sampling traces makes them useless for diagnostic and forensic use cases, requiring you to retain logs
* The complexity of sampling traces makes it more important to retain traditional metrics to corroborate results

From my perspective, **sampling and Observability 2.0 are fundamentally incompatible**.

I'm seeing some pretty exciting things happening to make logs/traces more cost-efficient, such as columnar databases like [Clickhouse](https://github.com/ClickHouse/ClickHouse), as well as other tracing projects like [Grafana Tempo](https://grafana.com/oss/tempo/) and [QuickWit](https://quickwit.io/). 

And I'm certainly not going to count out Honeycomb- knowing them, they'll continue to chip away at inefficiencies and find more ways to make their product more and more affordable, especially as they gain greater economies of scale.

#### Developer experience

The developer tools available for tracing are legitimately a lot more complex than those available for logs. We'll need ubiquitous tracing developer tools that approach the ease of use of `printf()` debugging, and that make it just as easy to validate that your tracing instrumentation is working the way you intended.

The developer experience for OTEL SDK configuration is still... a work in progress. I hope to see more projects that package up OTEL SDKs and provide easier installation and configuration. Or who knows, maybe some of the SDK owners will decide that 200 lines of boilerplate before you can send a single span is a bit much.

And we've got to find ways to deal with the edge cases (e.g capturing diagnostic data from crashes/panics) which OTEL tracing isn't currently handling.

#### Ecosystem and maturity

As long as most infrastructure and third-party tools are emitting only logs and metrics, it's going to be hard to go all in on tracing for first-party telemetry. It's possible that OpenTelemetry's trajectory will continue, and more and more OSS tools will start emitting traces, but there's a lot of ground to cover to hit critical mass.

As a litmus test: an Observability 2.0 product would need some sort of drop-in Kubernetes infrastructure monitoring solution similar to what you can get out-of-the-box with DataDog/NewRelic, or at least something comparable to the [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator), which are currently almost entirely driven by metrics.

## Being comfortable with incrementalism

Throughout my career, I've generally been biased towards incrementalism over revolutionary changes. I'm pretty stingy about my [innovation tokens](https://mcfunley.com/choose-boring-technology), and I tend to want to save them for things that will drive our core business strategy. Observability is something I want to keep boring and predictable.

Perhaps Observability 2.0 isn't something we should aim to achieve in short order. Rather, it's a philosophy we can apply in phases, in which we gradually get more and more value from our observability spend.

Right now, we're thinking about how to better use the three pillars we have:

* Seek out tools that integrate and visualize data from across multiple pillars (e.g. using exemplars to link metrics to traces)
* Look for ways to reduce the waste of overlapping, duplicate pillars 
* Try to utilize each pillar for its strengths, with tooling that uses them to complement each other
* Look for more sustainable ways to manage costs

Hopefully in a few years, the ecosystem will have evolved around the Observability 2.0 vision, and we'll be in a position to be a bit braver about our next steps.
